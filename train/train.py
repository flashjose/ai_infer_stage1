import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from model import SimpleCNN

# ============ è¶…å‚æ•°ï¼ˆDay4 ä¼šä¼˜åŒ–è¿™é‡Œï¼‰============
BATCH_SIZE = 64# æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 0.001# å­¦ä¹ ç‡
EPOCHS = 3# è®­ç»ƒè½®æ•°
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")# è®¾å¤‡
SAVE_PATH = "mnist_cnn.pth"

# ============ æ•°æ®åŠ è½½ ============
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))# æ•°æ®å½’ä¸€åŒ–
])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

# ============ æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨ ============
model = SimpleCNN().to(DEVICE)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# ============ è®­ç»ƒå¾ªç¯ ============
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
print(f"è®­ç»ƒå¼€å§‹...")

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(DEVICE), target.to(DEVICE)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)  # ğŸ‘ˆ loss åœ¨è¿™é‡Œç®—
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        if batch_idx % 200 == 0:
            print(f"Epoch {epoch + 1}/{EPOCHS} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}")

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1} å®Œæˆ | å¹³å‡ Loss: {avg_loss:.4f}")

# ============ ä¿å­˜æ¨¡å‹ ============
torch.save(model.state_dict(), SAVE_PATH)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {SAVE_PATH}")
